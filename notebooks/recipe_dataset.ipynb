{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZR5-gGGAuVi",
        "outputId": "1160271e-efe1-4bfb-952d-a4c7019054c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting icrawler\n",
            "  Downloading icrawler-0.6.10-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from icrawler) (4.13.4)\n",
            "Collecting bs4 (from icrawler)\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from icrawler) (5.4.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from icrawler) (11.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from icrawler) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from icrawler) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from icrawler) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->icrawler) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->icrawler) (4.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->icrawler) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->icrawler) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->icrawler) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->icrawler) (2025.7.14)\n",
            "Downloading icrawler-0.6.10-py3-none-any.whl (36 kB)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Installing collected packages: bs4, icrawler\n",
            "Successfully installed bs4-0.0.2 icrawler-0.6.10\n"
          ]
        }
      ],
      "source": [
        "pip install icrawler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBcep-WzFx3z"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import random\n",
        "from PIL import Image\n",
        "from icrawler.builtin import GoogleImageCrawler, BingImageCrawler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wt_TbUF7A8b_",
        "outputId": "27d73c8f-2690-46aa-9ad3-ff0e741550d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mqUbruhKJ2m"
      },
      "outputs": [],
      "source": [
        "\n",
        "ingredients = {\n",
        "    'egg': ['egg', 'raw egg', 'boiled egg'],\n",
        "    'banana': ['banana', 'sliced banana', 'banana bunch'],\n",
        "    'tomato': ['tomato', 'sliced tomato', 'tomato on table'],\n",
        "    'carrot': ['carrot', 'raw carrot', 'carrots on cutting board'],\n",
        "    'onion': ['onion', 'onion on cutting board', 'raw onion'],\n",
        "    'milk': ['milk', 'glass of milk', 'milk bottle'],\n",
        "    'flour': ['flour', 'flour in bowl', 'white flour'],\n",
        "    'lemon': ['lemon', 'lemon on cutting board', 'yellow lemon'],\n",
        "    'bread': ['bread', 'loaf of bread', 'whole wheat bread'],\n",
        "    'rice': ['rice', 'white rice', 'bowl of rice'],\n",
        "    'chicken-meat': ['raw chicken', 'raw chicken fillet', 'raw chicken breast'],\n",
        "    'potato': ['potato', 'sliced potato', 'potatoes in basket'],\n",
        "    'cheese': ['cheese', 'cheese block', 'sliced cheese'],\n",
        "    'pineapple': ['pineapple', 'whole pineapple', 'pineapple on table'],\n",
        "    'chocolate': ['chocolate', 'chocolate bar', 'dark chocolate']\n",
        "}\n",
        "\n",
        "raw_output_dir = 'raw-food-dataset'\n",
        "final_output_dir = '/content/drive/MyDrive/organized-food-dataset-fifteen'\n",
        "\n",
        "\n",
        "TARGET_IMAGES_PER_CLASS = 300\n",
        "MAX_PER_QUERY = 100\n",
        "\n",
        "def sanitize_folder_name(text):\n",
        "    return re.sub(r'[^a-zA-Z0-9_-]', '_', text.lower())\n",
        "\n",
        "def download_images():\n",
        "    os.makedirs(raw_output_dir, exist_ok=True)\n",
        "\n",
        "    for ingredient, queries in ingredients.items():\n",
        "        ingredient_dir = os.path.join(raw_output_dir, ingredient)\n",
        "        os.makedirs(ingredient_dir, exist_ok=True)\n",
        "\n",
        "        total_downloaded = 0\n",
        "        for query in queries:\n",
        "            if total_downloaded >= TARGET_IMAGES_PER_CLASS:\n",
        "                break\n",
        "\n",
        "            subfolder = os.path.join(ingredient_dir, sanitize_folder_name(query))\n",
        "            os.makedirs(subfolder, exist_ok=True)\n",
        "\n",
        "            remaining = TARGET_IMAGES_PER_CLASS - total_downloaded\n",
        "\n",
        "\n",
        "            google_crawler = GoogleImageCrawler(storage={'root_dir': subfolder})\n",
        "            google_crawler.crawl(keyword=query, max_num=min(MAX_PER_QUERY, remaining))\n",
        "\n",
        "\n",
        "            total_downloaded = count_images(ingredient_dir)\n",
        "            if total_downloaded >= TARGET_IMAGES_PER_CLASS:\n",
        "                break\n",
        "\n",
        "            if total_downloaded < TARGET_IMAGES_PER_CLASS:\n",
        "                remaining = TARGET_IMAGES_PER_CLASS - total_downloaded\n",
        "                bing_crawler = BingImageCrawler(storage={'root_dir': subfolder})\n",
        "                bing_crawler.crawl(keyword=query, max_num=min(MAX_PER_QUERY, remaining))\n",
        "\n",
        "            total_downloaded = count_images(ingredient_dir)\n",
        "\n",
        "\n",
        "def count_images(folder_path):\n",
        "    count = 0\n",
        "    for root, _, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                count += 1\n",
        "    return count\n",
        "\n",
        "def clean_corrupted_images(folder_path):\n",
        "    removed_count = 0\n",
        "    for root, _, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            filepath = os.path.join(root, file)\n",
        "            try:\n",
        "                img = Image.open(filepath)\n",
        "                img.verify()\n",
        "            except Exception:\n",
        "                os.remove(filepath)\n",
        "                removed_count += 1\n",
        "\n",
        "def merge_class_folders():\n",
        "    if os.path.exists(final_output_dir):\n",
        "        shutil.rmtree(final_output_dir)\n",
        "    os.makedirs(final_output_dir)\n",
        "\n",
        "    for class_name in os.listdir(raw_output_dir):\n",
        "        class_dir = os.path.join(raw_output_dir, class_name)\n",
        "        if not os.path.isdir(class_dir):\n",
        "            continue\n",
        "\n",
        "        target_class_dir = os.path.join(final_output_dir, 'all', class_name)\n",
        "        os.makedirs(target_class_dir, exist_ok=True)\n",
        "\n",
        "        for subfolder in os.listdir(class_dir):\n",
        "            subfolder_path = os.path.join(class_dir, subfolder)\n",
        "            if os.path.isdir(subfolder_path):\n",
        "                for img_file in os.listdir(subfolder_path):\n",
        "                    src = os.path.join(subfolder_path, img_file)\n",
        "                    dst = os.path.join(target_class_dir, img_file)\n",
        "\n",
        "                    base, ext = os.path.splitext(dst)\n",
        "                    i = 1\n",
        "                    while os.path.exists(dst):\n",
        "                        dst = f\"{base}_{i}{ext}\"\n",
        "                        i += 1\n",
        "\n",
        "                    shutil.copy2(src, dst)\n",
        "\n",
        "def split_dataset(split_ratios=(0.8, 0.1, 0.1)):\n",
        "    random.seed(42)\n",
        "\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        for class_name in ingredients.keys():\n",
        "            os.makedirs(os.path.join(final_output_dir, split, class_name), exist_ok=True)\n",
        "\n",
        "    for class_name in os.listdir(os.path.join(final_output_dir, 'all')):\n",
        "        class_folder = os.path.join(final_output_dir, 'all', class_name)\n",
        "        images = os.listdir(class_folder)\n",
        "        random.shuffle(images)\n",
        "\n",
        "        total = len(images)\n",
        "        train_end = int(total * split_ratios[0])\n",
        "        val_end = train_end + int(total * split_ratios[1])\n",
        "\n",
        "        splits = {\n",
        "            'train': images[:train_end],\n",
        "            'val': images[train_end:val_end],\n",
        "            'test': images[val_end:]\n",
        "        }\n",
        "\n",
        "        for split, files in splits.items():\n",
        "            for file in files:\n",
        "                src = os.path.join(class_folder, file)\n",
        "                dst = os.path.join(final_output_dir, split, class_name, file)\n",
        "                shutil.copy2(src, dst)\n",
        "\n",
        "    shutil.rmtree(os.path.join(final_output_dir, 'all'))\n",
        "\n",
        "download_images()\n",
        "clean_corrupted_images(raw_output_dir)\n",
        "merge_class_folders()\n",
        "clean_corrupted_images(final_output_dir)\n",
        "split_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.copytree('raw-food-dataset', '/content/drive/MyDrive/organized-food-dataset-fifteen', dirs_exist_ok=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "KfOYOyys5yda",
        "outputId": "a55b7347-c323-43e0-e1e8-54891d62b283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/cleaned-food-dataset-expanded-fifteen'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}